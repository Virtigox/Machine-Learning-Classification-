{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "outputs": [],
      "source": [
        "# Import libraries. You may or may not use all of these.\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "!wget https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv\n",
        "dataset = pd.read_csv('insurance.csv')\n",
        "dataset.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.isna().sum()"
      ],
      "metadata": {
        "id": "uawUon-bADtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this challenge, you will predict healthcare costs using a regression algorithm.\n",
        "\n",
        "You are given a dataset that contains information about different people including their healthcare costs. Use the data to predict healthcare costs based on new data.\n",
        "\n",
        "The first two cells of this notebook import libraries and the data.\n",
        "\n",
        "Make sure to convert categorical data to numbers. Use 80% of the data as the train_dataset and 20% of the data as the test_dataset.\n",
        "\n",
        "pop off the \"expenses\" column from these datasets to create new datasets called train_labels and test_labels. Use these labels when training your model.\n",
        "\n",
        "Create a model and train it with the train_dataset. Run the final cell in this notebook to check your model. The final cell will use the unseen test_dataset to check how well the model generalizes.\n",
        "\n",
        "To pass the challenge, model.evaluate must return a Mean Absolute Error of under 3500. This means it predicts health care costs correctly within $3500.\n",
        "\n",
        "The final cell will also predict expenses using the test_dataset and graph the results."
      ],
      "metadata": {
        "id": "h6wvxwu7ETU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparing\n",
        "\n",
        "CATEGORICAL_COLUMNS = ['sex','smoker','region']\n",
        "\n",
        "# converting string values to numerical data\n",
        "dataset = pd.get_dummies(dataset, columns = CATEGORICAL_COLUMNS)\n",
        "\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "YnXBRvLjLn3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset preparation before training and testing the model\n",
        "\n",
        "train_size = 0.8\n",
        "test_size = 0.2\n",
        "# train 80% test 20%\n",
        "raw_train_data = dataset.iloc[: int(len(dataset)*train_size)]\n",
        "raw_test_data = dataset.iloc[int(len(dataset)*train_size) : int(len(dataset)* (test_size + train_size))]\n",
        "\n",
        "# exclusion of 'expenses' column\n",
        "train_labels = raw_train_data['expenses']\n",
        "train_dataset = raw_train_data.drop(columns = 'expenses')\n",
        "\n",
        "test_labels = raw_test_data['expenses']\n",
        "test_dataset = raw_test_data.drop(columns = 'expenses')\n"
      ],
      "metadata": {
        "id": "KFiAgN14HW-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of epochs is a hyperparameter that determines how many times the model will go through the entire dataset. It's important to find an appropriate number of epochs to prevent underfitting (too few epochs) or overfitting (too many epochs).\n",
        "\n",
        "\n",
        "he batch size is a hyperparameter that determines the number of samples in each batch. Common batch sizes are powers of 2, such as 32, 64, or 128, but it can be adjusted based on the available memory and computational resources.\n",
        "\n",
        "If you have 1000 training samples and a batch size of 64, it will take 16 batches to complete one epoch."
      ],
      "metadata": {
        "id": "DtCtnFhX35Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1, activation='relu')\n",
        "  ])\n",
        "\n",
        "# Compile the model\n",
        "optimizer=\"Adam\"\n",
        "model.compile(loss='mae',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['mae','mse'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, train_labels, epochs=100, batch_size=32,verbose='auto',\n",
        "    callbacks=None,)\n"
      ],
      "metadata": {
        "id": "ffxgbjq841pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "V7w9dHc7PBCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OOw8g_UEPJh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "outputs": [],
      "source": [
        "## RUN THIS CELL TO TEST YOUR MODEL. DO NOT MODIFY CONTENTS.\n",
        "# Test model by checking how well the model generalizes using the test set.\n",
        "loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} expenses\".format(mae))\n",
        "\n",
        "if mae < 3500:\n",
        "  print(\"You passed the challenge. Great job!\")\n",
        "else:\n",
        "  print(\"The Mean Abs Error must be less than 3500. Keep trying.\")\n",
        "\n",
        "# Plot predictions.\n",
        "test_predictions = model.predict(test_dataset).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True values (expenses)')\n",
        "plt.ylabel('Predictions (expenses)')\n",
        "lims = [0, 50000]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims,lims)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}